{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Models - Customer Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cust_df = pd.read_csv(\"data/synth_customer_churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cust_df['ChurnCategory'].map({'Low Risk': 0, 'Medium Risk': 1, 'High Risk': 2})\n",
    "X = cust_df.drop(columns=['ChurnCategory', 'CustomerID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pipeline.binner import TenureBinner #Needed as our joblib contains a custom transformer\n",
    "preprocessor = joblib.load(\"models/preprocessing/preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data (avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('svc', SVC(probability=True)),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SMOTENN to pipeline to oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN\n",
    "import os\n",
    "\n",
    "original_counts = Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = {\n",
    "    1: int(original_counts[1] * 1.4),\n",
    "    2: int(original_counts[2] * 2.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smotenn = SMOTEENN(sampling_strategy=target_counts, \n",
    "                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),  \n",
    "    (\"resample\",smotenn), \n",
    "    (\"stacking_clf\", stacking_clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"models/classifiers\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "joblib.dump(stacking_pipeline, os.path.join(SAVE_DIR, \"stack_class_pipe.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'stacking_clf__rf__n_estimators': [100, 200],\n",
    "    'stacking_clf__svc__C': [0.1, 1.0],\n",
    "    'stacking_clf__gb__learning_rate': [0.05, 0.1],\n",
    "    'stacking_clf__final_estimator__C': [0.1, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "os.makedirs(\"models/configs\", exist_ok=True)\n",
    "with open(\"models/configs/stack_param_grid.json\", \"w\") as f:\n",
    "    json.dump(param_grid, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 - Create reusable base models in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    'rf': RandomForestClassifier(),\n",
    "    'svc': SVC(probability=True),\n",
    "    'gb' : GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "sampler = SMOTEENN(sampling_strategy=target_counts, \n",
    "                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in base_models.items():\n",
    "    pipelines[name] = Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"resample\", sampler),\n",
    "        (\"classifier\", model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "CLASS_PATH = \"models/classifiers/\"\n",
    "os.makedirs(CLASS_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pipe in pipelines.items():\n",
    "    joblib.dump(pipe, f\"{CLASS_PATH}/{name}_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(CLASS_PATH) if os.path.isfile(os.path.join(CLASS_PATH, f)) and not f.startswith(\"best_model\")]\n",
    "print(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-book-py-12-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
