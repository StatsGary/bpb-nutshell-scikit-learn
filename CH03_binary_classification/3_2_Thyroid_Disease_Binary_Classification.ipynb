{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7Y7l29vWipA"
   },
   "source": [
    " # Thyroid Disease Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCAecl0teHi5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tdf = pd.read_csv(\"data/MLDataR_thyroid_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "GxqDrAcpe9QM",
    "outputId": "43095a7f-20a5-4ca0-edff-8ad6ecf25ea4"
   },
   "outputs": [],
   "source": [
    "tdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1piHZtlmNmZ"
   },
   "source": [
    "## Exploratory Data Analysis  {EDA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "LsMnrOn8hJKd",
    "outputId": "12208069-65ea-42b1-f7df-e0310b4d0989"
   },
   "outputs": [],
   "source": [
    "from modelviz.histogram import plot_feature_histograms\n",
    "plot_feature_histograms(tdf, library='matplotlib', \n",
    "                        exclude_bin_encode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B56ScPKkj1HG"
   },
   "source": [
    "## Visualize missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bX9ha7Hblf0h"
   },
   "outputs": [],
   "source": [
    "from modelviz.missvals import plot_missing_values_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "wIbaFNRSnXaL",
    "outputId": "71ea914c-e508-4954-af3f-9b643648cbe7"
   },
   "outputs": [],
   "source": [
    "plot_missing_values_heatmap(tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bzZNMR9p6AW"
   },
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kcf3acLEtNg0"
   },
   "outputs": [],
   "source": [
    "from modelviz.relationships import plot_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "tp-ybSPctP5t",
    "outputId": "98ddefb9-36d3-48a9-a1e9-3b41cc023ec9"
   },
   "outputs": [],
   "source": [
    "plot_correlation_matrix(tdf, max_columns=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tdf['ThyroidClass']).value_counts().plot(\n",
    "    kind='bar', title='Class Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work on missing value volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_summarizer(df:pd.DataFrame, \n",
    "                              drop_cols:bool=False, \n",
    "                              drop_threshold:float=0.1, \n",
    "                              verbose=False):\n",
    "\n",
    "    missing_counts = df.isnull().sum()\n",
    "    non_missing_counts = df.notnull().sum()\n",
    "    total_vol = len(df)\n",
    "    missing_proportions = missing_counts / len(df)\n",
    "    non_missing_proportions = non_missing_counts / len(df)\n",
    "\n",
    "    props_df = pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"missing_count\": missing_counts,\n",
    "        \"non_missing_count\": non_missing_counts,\n",
    "        \"total_vol\": total_vol,\n",
    "        \"prop_missing\": missing_proportions,\n",
    "        \"prop_non_missing\": non_missing_proportions,\n",
    "        'data_type': df.dtypes.values\n",
    "    })\n",
    "    props_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    if drop_cols:\n",
    "        cols_to_drop=props_df[props_df['prop_missing'] > drop_threshold]['feature'].tolist()\n",
    "        df_resized = df.drop(columns=cols_to_drop)\n",
    "        if verbose:\n",
    "            print(f'Dropping columns: {cols_to_drop}')\n",
    "    else:\n",
    "        df_resized = df.copy()\n",
    "    \n",
    "    return props_df, df_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df, res_df = missing_values_summarizer(tdf, \n",
    "                                             drop_cols=True, \n",
    "                                             drop_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.drop(columns=['TSH_reading'], \n",
    "            axis=1, \n",
    "            inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns with Zero Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = res_df.var(numeric_only=True)\n",
    "zero_variance_columns = variances[variances == 0].index.tolist()\n",
    "print(zero_variance_columns)\n",
    "res_df.drop(columns=zero_variance_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = res_df.drop(columns='ThyroidClass', axis=1)\n",
    "y = pd.Series(res_df['ThyroidClass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'negative': 0,\n",
    "    'sick': 1\n",
    "}\n",
    "\n",
    "y_mapped = y.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y_mapped, \n",
    "                                                    stratify=y_mapped, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_sizes(X_train, X_test):\n",
    "    X_train_size = len(X_train)\n",
    "    X_train_cols = len(X_train.columns)\n",
    "\n",
    "    X_test_size = len(X_test)\n",
    "    X_test_cols = len(X_train.columns)\n",
    "\n",
    "    if X_train_cols != X_test_cols:\n",
    "        assert ValueError('Columns in both DataFrames should match, as X_train has: {X_train_cols} columns and X_test has: {X_test_cols} columns')\n",
    "\n",
    "    print(\n",
    "        f\"Train and testing set statistics\\n\\n\"\n",
    "        f\"X_train size: {X_train_size} | X_train columns: {X_train_cols}\\n\"\n",
    "        f\"X_test size: {X_test_size} | X_test columns: {X_test_cols}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_split_sizes(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot encode `ref_src` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_columns(X_train, X_test, categorical_columns, drop='first'):\n",
    "    encoder = OneHotEncoder(sparse_output=False, \n",
    "                            handle_unknown='ignore', \n",
    "                            drop=drop)\n",
    "    encoder.fit(X_train[categorical_columns])\n",
    "    X_train_encoded_array = encoder.transform(X_train[categorical_columns]).astype(float)\n",
    "    X_test_encoded_array = encoder.transform(X_test[categorical_columns]).astype(float)\n",
    "    encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "    X_train_encoded_df = pd.DataFrame(X_train_encoded_array, \n",
    "                                      columns=encoded_feature_names, \n",
    "                                      index=X_train.index)\n",
    "    X_test_encoded_df = pd.DataFrame(X_test_encoded_array, \n",
    "                                     columns=encoded_feature_names, \n",
    "                                     index=X_test.index)\n",
    "    print(\"Encoded Data Example (Train):\")\n",
    "    print(X_train_encoded_df.head())\n",
    "\n",
    "    X_train = X_train.drop(columns=categorical_columns)\n",
    "    X_test = X_test.drop(columns=categorical_columns)\n",
    "  \n",
    "    X_train_encoded = pd.concat([X_train.reset_index(drop=True), \n",
    "                                 X_train_encoded_df.reset_index(drop=True)], axis=1)\n",
    "    X_test_encoded = pd.concat([X_test.reset_index(drop=True), \n",
    "                                X_test_encoded_df.reset_index(drop=True)], axis=1)\n",
    "    return X_train_encoded, X_test_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohenc, X_test_ohenc = encode_categorical_columns(\n",
    "    X_train=X_train, \n",
    "    X_test=X_test, \n",
    "    categorical_columns=['ref_src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MICE to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_simple = False\n",
    "if use_simple:\n",
    "    imputer = SimpleImputer()\n",
    "else:\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "imputed_data = imputer.fit_transform(X_train_ohenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform both training and testing data\n",
    "X_train_imputed = imputer.transform(X_train_ohenc)\n",
    "X_test_imputed = imputer.transform(X_test_ohenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if np.isnan(X_train_imputed).any() | np.isnan(X_test_imputed).any():\n",
    "    print('Null values remain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over and under sampling techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Type**         | **Python Package**      | **Description**                                                                 |\n",
    "|-------------------|-------------------------|---------------------------------------------------------------------------------|\n",
    "| **SMOTE**        | `imbalanced-learn`      | Synthetic Minority Oversampling Technique. Generates synthetic samples by interpolating between existing minority samples. Suitable for numeric datasets. |\n",
    "| **ROS (Random Over-Sampling)** | `imbalanced-learn`      | Replicates samples from the minority class to balance the dataset. Simple but can lead to overfitting. |\n",
    "| **ADASYN**       | `imbalanced-learn`      | Adaptive Synthetic Sampling. Focuses on generating synthetic samples in regions where the minority class is sparse. Variant of SMOTE. |\n",
    "| **ROSE (Random Over-Sampling Examples)** | No direct Python implementation (alternatives: custom implementation, ROS, ADASYN) | Adds noise to oversampled data points to create diverse synthetic examples. Native to R; Python alternatives include ROS and ADASYN. |\n",
    "| **SMOTEENN**     | `imbalanced-learn`      | Combination of SMOTE (oversampling) and Edited Nearest Neighbors (undersampling). Balances the dataset and removes noisy samples. |\n",
    "| **SMOTETomek**   | `imbalanced-learn`      | Combines SMOTE with Tomek Links (undersampling technique). Removes borderline samples after oversampling. |\n",
    "| **Random Under-Sampling** | `imbalanced-learn`      | Reduces the dataset by removing samples from the majority class to balance the dataset. Can lead to loss of important information. |\n",
    "| **Cluster-Based Over-Sampling** | Custom implementation required | Uses clustering (e.g., k-means) to create synthetic samples by identifying dense regions in the feature space. Experimental technique. |\n",
    "| **Class Weight Adjustment** | `scikit-learn`          | Adjusts model training weights to penalize misclassification of the minority class more heavily. Does not change the dataset but impacts model training. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization - Scaling our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scale = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = robust_scale.fit_transform(X_train_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = robust_scale.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(X_train_scaled).any(),\"NaN values found in X_train_pp.\"\n",
    "assert not np.isnan(X_test_scaled).any(), \"NaN values found in X_test_pp.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "k = 15  \n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "selected_features = selector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "print(scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"logistic_regression\": LogisticRegression(max_iter=2000, \n",
    "                                              random_state=42, \n",
    "                                              class_weight='balanced'),\n",
    "    \"svc\": SVC(kernel='rbf', probability=True, \n",
    "               random_state=42, \n",
    "               class_weight='balanced'),\n",
    "\n",
    "    \"cat_boost\": CatBoostClassifier(iterations=500, \n",
    "                                    depth=6, \n",
    "                                    learning_rate=0.1, \n",
    "                                    class_weights=[1, 10], \n",
    "                                    verbose=0),\n",
    "\n",
    "    \"xgboost_scaled\": XGBClassifier(n_estimators=500, \n",
    "                                    max_depth=6, \n",
    "                                    learning_rate=0.1, \n",
    "                                    scale_pos_weight=scale_pos_weight, \n",
    "                                    random_state=42),\n",
    "\n",
    "    \"xgboost_unscaled\": XGBClassifier(n_estimators=500, \n",
    "                                      max_dept=6, \n",
    "                                      learning_rate=0.1, \n",
    "                                      random_state=42),\n",
    "\n",
    "    \"LGBM\": lgb.LGBMClassifier(n_estimators=500,\n",
    "                               learning_rate=0.1,\n",
    "                               class_weight='balanced',\n",
    "                               random_state=42, \n",
    "                               max_depth=6)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, \n",
    "                      shuffle=True, \n",
    "                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train_selected\n",
    "X_test_final = X_test_selected\n",
    "y_train_final = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_pred_proba):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_simple_confusion_matrix(cm, classes, \n",
    "                                 model_name, normalize=False, \n",
    "                                 cmap='Blues'):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "        title = f'Normalized Confusion Matrix for {model_name}'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "        title = f'Confusion Matrix for {model_name}'\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap, cbar=False,\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelviz.confusion_matrix import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "fitted_models = {}\n",
    "model_scores = {}\n",
    "threshold_results = {}\n",
    "EVAL_METRIC = 'recall' \n",
    "ADJUSTED_THRESHOLD = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Perform cross-validation with SMOTEENN included in the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('smoteenn', SMOTEENN(random_state=42, sampling_strategy=0.5)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    cv_scores = cross_val_score(pipeline, X_train_final, \n",
    "        y_train_final, cv=skf, scoring=EVAL_METRIC)\n",
    "    \n",
    "    # Output cross-validation scores\n",
    "    mean_score = cv_scores.mean()\n",
    "    std_score = cv_scores.std()\n",
    "    model_scores[model_name] = {'mean_score': mean_score, \n",
    "                                'std_score': std_score}\n",
    "    print(f\"{model_name} Mean {EVAL_METRIC} Score: {mean_score:.4f} (+/- {std_score:.4f})\\n\")\n",
    "    \n",
    "    # Apply SMOTEENN to the entire training data\n",
    "    smoteenn = SMOTEENN(random_state=42, \n",
    "                        sampling_strategy=0.5)\n",
    "    X_resampled, y_resampled = smoteenn.fit_resample(X_train_final, y_train_final)\n",
    "    \n",
    "    # Clone the model to avoid reusing the one from cross-validation\n",
    "    model_clone = clone(model)\n",
    "    \n",
    "    # Fit the model on the resampled training data\n",
    "    model_clone.fit(X_resampled, y_resampled)\n",
    "    fitted_models[model_name] = model_clone\n",
    "    \n",
    "    # Predict probabilities on the test data (without resampling)\n",
    "    y_pred_proba = model_clone.predict_proba(X_test_final)[:, 1]\n",
    "    \n",
    "    # Adjust predictions using the new threshold\n",
    "    y_pred_adjusted = (y_pred_proba >= ADJUSTED_THRESHOLD).astype(int)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    metrics = compute_metrics(y_test, y_pred_adjusted, y_pred_proba)\n",
    "    \n",
    "    # Store metrics\n",
    "    threshold_results[model_name] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_results['cat_boost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelviz.roc import plot_roc_curve_with_youdens_thresholds\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, X_test, y_test, \n",
    "                   adjusted_threshold=0.5, \n",
    "                   classes=['Negative', 'Sick'], \n",
    "                   calculate_youdens=False, plot_simple_cm=False):\n",
    "    print(f\"Evaluating {model_name} on test data...\")\n",
    "\n",
    "    # Predict probabilities and apply adjusted threshold\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_adjusted = (y_pred_proba >= adjusted_threshold).astype(int)\n",
    "    adjusted_metrics = compute_metrics(y_test, y_pred_adjusted, y_pred_proba)\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, \n",
    "                                     y_pred_proba)\n",
    "\n",
    "    evaluation_results = {\n",
    "        'fitted_model': model,\n",
    "        'adjusted_threshold': adjusted_threshold,\n",
    "        'metrics_at_adjusted_threshold': adjusted_metrics,\n",
    "        'roc_curve': (fpr, tpr, thresholds),\n",
    "    }\n",
    "\n",
    "    if calculate_youdens:\n",
    "        # Find optimal threshold using Youden's J statistic\n",
    "        youdens_j = tpr - fpr\n",
    "        optimal_idx = np.argmax(youdens_j)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        max_youdens_j = youdens_j[optimal_idx]\n",
    "        print(f\"Maximum Youden's J for {model_name}: {max_youdens_j:.4f} at threshold {optimal_threshold:.4f}\\n\")\n",
    "        # Apply optimal threshold and compute metrics\n",
    "        y_pred_youden = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        youden_metrics = compute_metrics(\n",
    "            y_test, \n",
    "            y_pred_youden, \n",
    "            y_pred_proba)\n",
    "\n",
    "        # Update evaluation results with Youden's metrics\n",
    "        evaluation_results.update({\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'metrics_at_optimal_threshold': youden_metrics,\n",
    "            'max_youdens_j': max_youdens_j\n",
    "        })\n",
    "\n",
    "        # Plot Confusion Matrix for Youden's threshold\n",
    "        plot_confusion_matrix(\n",
    "            youden_metrics['confusion_matrix'], \n",
    "            classes=classes,\n",
    "            model_name=f\"{model_name} (Youden's J Threshold)\",\n",
    "            table_fontsize=8)\n",
    "\n",
    "    if plot_simple_cm:\n",
    "        plot_simple_confusion_matrix(\n",
    "            adjusted_metrics['confusion_matrix'], \n",
    "            classes=classes,\n",
    "            model_name=f\"{model_name} (Adjusted Threshold)\")\n",
    "        \n",
    "    else: \n",
    "        # Plot Confusion Matrix for Adjusted Threshold\n",
    "        plot_confusion_matrix(\n",
    "            adjusted_metrics['confusion_matrix'], classes=classes,\n",
    "            model_name=f\"{model_name} (Adjusted Threshold)\",\n",
    "            table_fontsize=8)\n",
    "    # Plot ROC Curve with thresholds\n",
    "    plot_roc_curve_with_youdens_thresholds(\n",
    "        fpr, tpr, \n",
    "        thresholds, \n",
    "        adjusted_metrics['roc_auc'],\n",
    "        model_name, \n",
    "        adjusted_threshold, \n",
    "        optimal_threshold if calculate_youdens else None,\n",
    "        xlabel='False Positive Rate (1-FPR)',\n",
    "        ylabel='True Positive Rate (TPR)')\n",
    "\n",
    "    return evaluation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your adjusted threshold\n",
    "ADJUSTED_THRESHOLD = 0.5  \n",
    "# Dictionary to store all evaluation results\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for model_name, model in fitted_models.items():\n",
    "\n",
    "    evaluation_results = evaluate_model(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        X_test=X_test_final,\n",
    "        y_test=y_test,\n",
    "        adjusted_threshold=ADJUSTED_THRESHOLD,\n",
    "        calculate_youdens=True, \n",
    "        plot_simple_cm=False\n",
    "    )\n",
    "    \n",
    "    all_evaluation_results[model_name] = evaluation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evaluation_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sklearn-book-py-12-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
